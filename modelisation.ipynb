{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import nltk\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 1 Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"bag\": False,\n",
    "    \"tfidf\": False,\n",
    "    \"unsupervised\": True,\n",
    "    \"supervised\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 2 Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/data_cleaned.csv\", index_col=\"Id\")\n",
    "\n",
    "data[\"Tags\"] = data[\"Tags\"].apply(eval)\n",
    "# data[\"Tokens\"] = data[\"Tokens\"].apply(eval)\n",
    "# data[\"POS\"] = data[\"POS\"].apply(eval)\n",
    "# data[\"Lemmatized\"] = data[\"Lemmatized\"].apply(eval)\n",
    "# data[\"LemmaAndStem\"] = data[\"LemmaAndStem\"].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 3 Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[[\"Tags\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Tags.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = []\n",
    "for row in data.Tags.values:\n",
    "    tags += row\n",
    "tags = list(set(tags))\n",
    "tags[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 4 Bag-Of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(dataset, max_features=None, min_df=0.0, max_df=1.0):\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=None, preprocessor=None, stop_words=None, max_features=max_features, max_df=max_df, min_df=min_df)\n",
    "    matrix = vectorizer.fit_transform(dataset).toarray()\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "    bag = pd.DataFrame(data=matrix, columns=vocab)\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "if config[\"bag\"]:\n",
    "    bag = bow(data, \"Sentence\")\n",
    "    display = bag.iloc[:5, :20]\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 5 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(dataset, feature, max_features=None):\n",
    "    vectorizer = TfidfVectorizer(tokenizer=None, stop_words=None, max_features=max_features)\n",
    "    matrix = vectorizer.fit_transform(dataset[feature]).toarray()\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "    tfidf = pd.DataFrame(data=matrix, columns=vocab)\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = None\n",
    "if config[\"tfidf\"]:\n",
    "    tfidf = tfidf(data, \"Sentence\")\n",
    "    display = tfidf.iloc[:5, :20]\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 6 Unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_dirichlet_allocation(dataset: pd.DataFrame, n_topics: int, max_iter=5, learning_offset=50, max_features=None):\n",
    "    feature_names = dataset.columns\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, max_iter=max_iter, learning_method=\"online\", learning_offset=learning_offset, random_state=0)\n",
    "    lda.fit(data)\n",
    "    return lda, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.mlreview.com/topic-modeling-with-scikit-learn-e80d33668730\n",
    "#\n",
    "def display_topics(model, feature_names, no_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}\")\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/#9buildldamodelwithsklearn\n",
    "#\n",
    "def latent_dirichlet_allocation_tuning(dataset: pd.DataFrame, param_grid: dict):\n",
    "    data_bow = bow(dataset, min_df=.005)\n",
    "    feature_names = data_bow.columns\n",
    "\n",
    "    lda = LatentDirichletAllocation()\n",
    "    gs = GridSearchCV(lda, param_grid)\n",
    "    gs.fit(data_bow)\n",
    "\n",
    "    return gs, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 6.1 Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 6.2 Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bow = bow(data[\"Sentence\"], min_df=.005, max_df=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bow.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"unsupervised\"]:\n",
    "    param_grid = {\n",
    "        \"n_components\": [10],\n",
    "        \"learning_decay\": [.7],\n",
    "        \"random_state\": [0],\n",
    "        \"n_jobs\": [10]\n",
    "    }\n",
    "\n",
    "    latent_dirichlet_allocation_tuning(data[\"Sentence\"], param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda, feature_names = lda(data, \"Sentence\", FEATURE_EXTRACTION.TFIDF, 20, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_topics(lda, feature_names, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 7 Supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0 Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scree_plot(dataset, figsize=(15, 5)):\n",
    "    pca = PCA()\n",
    "    pca.fit(dataset)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    explain_variance = pd.Series(pca.explained_variance_ratio_)\n",
    "    explain_variance.plot(kind=\"bar\", alpha=0.7)\n",
    "\n",
    "    total = 0\n",
    "    var_ls = []\n",
    "    for x in explain_variance:\n",
    "        total = total + x\n",
    "        var_ls.append(total)\n",
    "\n",
    "    pd.Series(var_ls).plot(marker=\"o\", alpha=0.7)\n",
    "    plt.xlabel(\"Principle Components\", fontsize=\"x-large\")\n",
    "    plt.ylabel(\"Percentage Variance Explained\", fontsize=\"x-large\")\n",
    "    plt.title(\"Scree plot\", fontsize=\"xx-large\")\n",
    "    plt.show()\n",
    "\n",
    "    return pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(dataset, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_data = pca.fit_transform(dataset)\n",
    "    components_name = [f\"PC{i+1}\" for i in range(pca_data.shape[1])]\n",
    "    pca_data = pd.DataFrame(data=pca_data, columns=components_name)\n",
    "    loadings = pd.DataFrame(\n",
    "        data=pca.components_.T,\n",
    "        columns=components_name,\n",
    "        index=dataset.columns)\n",
    "    return pca, pca_data, loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.codementor.io/@agarrahul01/multiclass-classification-using-random-forest-on-scikit-learn-library-hkk4lwawu\n",
    "# https://www.kaggle.com/patrickaudriaz/random-forests-for-multiclass-classification\n",
    "# \n",
    "def classifier_tuning(dataset: pd.DataFrame, model, param_grid: dict):\n",
    "    X = tfidf(dataset, \"Sentence\")\n",
    "    y = ?\n",
    "    feature_names = X.columns\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "    gs = GridSearchCV(model, param_grid)\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    return gs, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 7.1 PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"supervised\"]:\n",
    "    data_tfidf = tfidf(data, \"Sentence\")\n",
    "    pca = scree_plot(data_tfidf.iloc[:, :20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 7.2 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"supervised\"]:\n",
    "    param_grid = {\n",
    "        \"n_estimators\": [10],\n",
    "        \"criterion\": [\"entropy\"],\n",
    "        \"random_state\": [0],\n",
    "    }\n",
    "\n",
    "    model = RandomForestClassifier()\n",
    "    gs, feature_names = classifier_tuning(data, model, param_grid)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "586ad1ed5c97141e2437e681efbf1ec0adcd17d830cf5af2ca3d2819e743e158"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
